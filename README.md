# Multi-agent-exploration-2D
                                      Multi agent exploration on unknown maps using Reinforcement Learning.








Modern mobile robots have begun to be used in many exploration and exploration and rescue applications. They are essentially coordinated by human operators and work with inspection or rescue teams. Over time, robots (agents) have become more sophisticated and we see more autonomy in more complex environments.
Therefore, the purpose of this diploma thesis is to present an approach for autonomous multi-agent coordination for exploring and covering unknown environments. The method we suggest is reinforcement learning in combination with the use of neural networks (Deep Learning) to plan the course for each agent separately and also the effort to achieve collaborative behavior between them.
Specifically, we have used two techniques applied in recent years, which are the target neural network and the prioritized experience replay, which have been proven to stabilize and accelerate the training process. Also, agents must avoid obstacles (walls) throughout the exploration. We have also solved the problem of the need for prior information / knowledge about the environment, thus using only the local information available at any given time to make the decision of each agent. Also, in addition to the route created for exploring the maps, agents successfully avoided obstacles (walls). In this diploma thesis, the exploration of the unknown environment is done in a two-dimensional model (2D) using multiple agent for a variety of different maps, ranging from small sizes to and large sizes. Also, the map that our model is checked will be either randomly made or in a specific model of a real cave. Finally, the efficiency of the exploration is investigated for a different number of agents and for a different type of neural network.
